<p xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" id="Par26">No statistical method was used to predetermine sample size, as there are no standardised calculations for machine learning. For consistency, all implemented approaches were evaluated using the same QC and random train-test splits of the data (Fig.&#160;<xref rid="Fig1" ref-type="fig">1</xref>) which were well balanced for case-control status, age-at-baseline or assessment, sex, genotyping centre, and the distribution of all principal components (Supplementary Fig.&#160;<xref rid="MOESM1" ref-type="media">2</xref>). Participants were randomly separated into 70&#8211;30% train-test splits, with the same split applied to all algorithms, resulting in 29,180 individuals in the training set (14,006 cases; 15,174 controls) and 12,506 for testing (6007 cases; 6499 controls), each with 215,193 predictors after quality control procedures. ML models were built with and without SNPs from the <italic>APOE</italic> region (Chr19:44.4&#8211;46.5&#8201;Mb). In training ML and PRS models, analyses were adjusted for covariates comprising genetic sex, 20 principal component (PCs), and genotyping centre. The adjustment method was altered to be appropriate for each modelling approach: covariates were included in the final layer for NNs, with predictions and importance scores taken from non-covariate nodes (see Supplementary Fig.&#160;<xref rid="MOESM1" ref-type="media">3</xref>); for GBMs and MB-MDR, covariates were z-transformed and then regressed-off from the data before modelling. All reported area under the receiver operator characteristic curve (AUC) values are calculated on the predicted probabilities from models (without thresholding) and adjusted again for confounders in the test split. We utilise penalisation and cross-validated random (GBMs) or grid-based (NNs and MB-MDR) hyperparameter search to reduce the likelihood of overfitting. Details for training and covariate adjustment are given in Supplementary sections&#160;<xref rid="MOESM1" ref-type="media">1.2</xref>-<xref rid="MOESM1" ref-type="media">1.4</xref>. To ensure robust results, stability was assessed by re-running all models on three additional random 70&#8211;30% train-test splits of the data.</p>