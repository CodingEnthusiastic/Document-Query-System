<p xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" id="Par27">GBMs were trained using version 1.7.6 of the XGBoost package<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>, an efficient implementation of regularised gradient boosting, and the dask package<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>, version 2023.1.1, which allows for distributed training of ML models across multiple nodes in a high-performance computing (HPC) cluster. Hyperparameters for learning rate, tree depth, and column sampling fraction were tuned on a random subsample of the training set using random search (Supplementary section&#160;<xref rid="MOESM1" ref-type="media">1.2</xref>). Importance of all predictors in GBMs was assessed using SHapley Additive exPlanatory (SHAP) values<sup><xref ref-type="bibr" rid="CR57">57</xref>,<xref ref-type="bibr" rid="CR58">58</xref></sup>.</p>